{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import math\n",
    "import os\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torchtext\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify if CUDA is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If CUDA is available print devices\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA devices:')\n",
    "    for device in range(0, torch.cuda.device_count()):\n",
    "        print('\\t{} - {}'.format(device, torch.cuda.get_device_name(device)))\n",
    "else:\n",
    "    print('No CUDA devices')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self, embedding_size, encoding_size, use_cuda=False):\n",
    "    super(Encoder, self).__init__()\n",
    "\n",
    "    self.encoding_size = encoding_size\n",
    "   \n",
    "    self.encoder = nn.LSTM(\n",
    "      input_size=embedding_size, \n",
    "      hidden_size=encoding_size,\n",
    "      num_layers=1,\n",
    "      bias=True,\n",
    "      batch_first=True,\n",
    "      dropout=0.1,\n",
    "      bidirectional=False\n",
    "    )\n",
    "  \n",
    "  def forward(self, embeddings):\n",
    "    _, (hidden, memory) = self.encoder(embeddings)\n",
    "    return hidden, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "  def __init__(self, encoding_size, embedding_size, use_cuda=False):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "\n",
    "    self.encoding_size = encoding_size\n",
    "\n",
    "    self.decoder = nn.LSTM(\n",
    "      input_size=embedding_size, \n",
    "      hidden_size=encoding_size,\n",
    "      num_layers=1,\n",
    "      bias=True,\n",
    "      batch_first=True,\n",
    "      dropout=0.1,\n",
    "      bidirectional=False\n",
    "    )\n",
    "\n",
    "    self.dim_linear = nn.Linear(encoding_size, embedding_size)\n",
    "    self.dim_fn = nn.Tanh()    \n",
    "\n",
    "  def forward(self, embeddings, init_hidden, init_memory):\n",
    "\n",
    "    output_, (_, _) = self.decoder(embeddings, (init_hidden, init_memory))\n",
    "    linear = self.dim_linear(output_)\n",
    "    \n",
    "    return self.dim_fn(linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining functions related with transforming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform each sentence by adding a start and finish sequence for encoder and decoder training\n",
    "def get_transformation(vocabulary, embedding_size):\n",
    "    first_embedding = torch.ones((embedding_size,))\n",
    "    last_embedding = torch.zeros((embedding_size,))\n",
    "\n",
    "    def transform_example(example):\n",
    "        transformed = []\n",
    "        transformed.append(first_embedding)\n",
    "\n",
    "        for idx in example:\n",
    "            transformed.append(vocabulary.vectors[idx])\n",
    "        \n",
    "        transformed.append(last_embedding)\n",
    "        \n",
    "        transformed = torch.stack(transformed)[:15]\n",
    "        transformed = transformed.unsqueeze(0)\n",
    "\n",
    "        return transformed\n",
    "\n",
    "    return transform_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices from word's vectors\n",
    "def get_index_fn(vectors):\n",
    "    def get_index(prediction):\n",
    "        indices = []\n",
    "\n",
    "        for vector in prediction:\n",
    "            result = torch.abs(vectors - vector).norm(2, dim=1)\n",
    "            indices.append(torch.argmin(result))\n",
    "\n",
    "        indices = torch.stack(indices)\n",
    "        return indices\n",
    "\n",
    "    return get_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function recover a sentence from word's indices\n",
    "def get_text_fn(itos):\n",
    "    def get_text(example):\n",
    "        text = []\n",
    "        for idx in example:\n",
    "            text.append(itos[idx])\n",
    "\n",
    "        return ' '.join(text)\n",
    "    return get_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define variables related with loading information and training/validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTORS_LOADED = 20000\n",
    "VAL_PARTITION = 0.05 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading word vectors and trainig/validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext = torchtext.vocab.FastText(language='en', max_vectors=VECTORS_LOADED, cache='../.vector_cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = torchtext.vocab.Vocab(collections.Counter(fasttext.stoi.keys()))\n",
    "vocabulary.set_vectors(fasttext.stoi, fasttext.vectors, fasttext.dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = torchtext.datasets.AG_NEWS(ngrams=3, vocab=vocabulary, root='../.data')\n",
    "test_dataset = test_dataset[:int(len(test_dataset) * VAL_PARTITION)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining variables related with training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "EMBEDDING_SIZE = fasttext.dim\n",
    "ENCODING_SIZE = 1024\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining training components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(EMBEDDING_SIZE, ENCODING_SIZE, use_cuda=USE_CUDA)\n",
    "decoder = Decoder(ENCODING_SIZE, EMBEDDING_SIZE, use_cuda=USE_CUDA)\n",
    "\n",
    "encoder.load_state_dict(torch.load('checkpoints/encoder.pt'))\n",
    "decoder.load_state_dict(torch.load('checkpoints/decoder.pt'))\n",
    "\n",
    "if USE_CUDA:\n",
    "    encoder = encoder.cuda()\n",
    "    decoder = decoder.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    list(encoder.parameters()) + list(decoder.parameters()), \n",
    "    lr=LEARNING_RATE\n",
    ")\n",
    "\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation_fn = get_transformation(vocabulary, EMBEDDING_SIZE)\n",
    "get_text = get_text_fn(vocabulary.itos)\n",
    "get_indices = get_index_fn(vocabulary.vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(encoder, decoder, loss_fn, optimizer, batch, use_cuda):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    if use_cuda:\n",
    "      batch = batch.cuda()\n",
    "\n",
    "    encoder.zero_grad()\n",
    "    decoder.zero_grad()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    representation, memory = encoder(batch[:,1:,:])\n",
    "\n",
    "    if use_cuda:\n",
    "      representation = representation.cuda()\n",
    "      memory = memory.cuda()\n",
    "    \n",
    "    decodings = decoder(batch[:,:-1,:], representation, memory)\n",
    "\n",
    "    loss = loss_fn(batch[:,1:,:], decodings)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_step(encoder, decoder, loss_fn, batch, use_cuda):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        if use_cuda:\n",
    "            batch = batch.cuda()\n",
    "\n",
    "        representation, memory = encoder(batch[:,1:,:])\n",
    "\n",
    "        if use_cuda:\n",
    "            representation = representation.cuda()\n",
    "            memory = memory.cuda()\n",
    "        \n",
    "        decodings = decoder(batch[:,:-1,:], representation, memory)\n",
    "\n",
    "        loss = loss_fn(batch[:,1:,:], decodings)\n",
    "\n",
    "        return loss.item(), decodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Define steps where examples will be sampled \n",
    "example_step = VAL_PARTITION * len(train_dataset)\n",
    "test_examples = iter(test_dataset)\n",
    "\n",
    "last_val_loss = None\n",
    "\n",
    "# For EPOCHS\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "  # Restart train and validation datasets\n",
    "  examples = iter(train_dataset)\n",
    "  val_examples = iter(test_dataset)\n",
    "\n",
    "  # Progress bar for training dataset\n",
    "  progress_bar = tqdm(range(len(train_dataset)))\n",
    "  train_loss = 0\n",
    "  \n",
    "  # For all data in training dataset\n",
    "  \n",
    "  for batch_idx in progress_bar:\n",
    "\n",
    "    # Add train loss to progress bar\n",
    "    progress_bar.set_description('Loss: {}'.format(train_loss / (batch_idx + 1)))\n",
    "    \n",
    "    # Train step\n",
    "    example = next(examples)[1]\n",
    "    embeddings = transformation_fn(example)\n",
    "\n",
    "    train_loss += train_step(encoder, decoder, loss_fn, optimizer, embeddings, USE_CUDA)\n",
    "\n",
    "\n",
    "    if batch_idx % example_step == 0:\n",
    "      with torch.no_grad():\n",
    "        try:\n",
    "          example = next(test_examples)[1]\n",
    "        except:\n",
    "          test_examples = iter(test_dataset)\n",
    "          example = next(test_examples)[1]\n",
    "\n",
    "        embeddings = transformation_fn(example)\n",
    "\n",
    "        _, decodings = val_step(encoder, decoder, loss_fn, embeddings, USE_CUDA)\n",
    "\n",
    "        print('\\nReal: {}'.format(get_text(example[:15])))\n",
    "        print('Decoded: {}'.format(get_text(get_indices(decodings[0].cpu()))))      \n",
    "\n",
    "\n",
    "  with torch.no_grad():\n",
    "    progress_bar = tqdm(range(len(test_dataset)))\n",
    "    val_loss = 0\n",
    "\n",
    "    for batch_idx in progress_bar:\n",
    "      progress_bar.set_description('Val loss: {}'.format(val_loss / (batch_idx + 1)))\n",
    "      example = next(val_examples)[1]\n",
    "      embeddings = transformation_fn(example)\n",
    "\n",
    "      val_loss += val_step(encoder, decoder, loss_fn, embeddings, USE_CUDA)[0]\n",
    "       \n",
    "\n",
    "    if last_val_loss is None or val_loss < last_val_loss:\n",
    "      last_val_loss = val_loss\n",
    "\n",
    "      torch.save(encoder.state_dict(), 'checkpoints/encoder_{}.pt'.format(epoch))\n",
    "      torch.save(decoder.state_dict(), 'checkpoints/decoder_{}.pt'.format(epoch))"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
