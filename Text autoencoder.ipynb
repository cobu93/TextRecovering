{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import math\n",
    "import os\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torchtext\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify if CUDA is available"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CUDA devices:\n\t0 - GeForce RTX 2060\n"
    }
   ],
   "source": [
    "# If CUDA is available print devices\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA devices:')\n",
    "    for device in range(0, torch.cuda.device_count()):\n",
    "        print('\\t{} - {}'.format(device, torch.cuda.get_device_name(device)))\n",
    "else:\n",
    "    print('No CUDA devices')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define encoder and decoder"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self, embedding_size, encoding_size, use_cuda=False):\n",
    "    super(Encoder, self).__init__()\n",
    "\n",
    "    self.encoding_size = encoding_size\n",
    "   \n",
    "    self.encoder = nn.LSTM(\n",
    "      input_size=embedding_size, \n",
    "      hidden_size=encoding_size,\n",
    "      num_layers=1,\n",
    "      bias=True,\n",
    "      batch_first=True,\n",
    "      dropout=0.1,\n",
    "      bidirectional=False\n",
    "    )\n",
    "  \n",
    "  def forward(self, embeddings):\n",
    "    _, (hidden, memory) = self.encoder(embeddings)\n",
    "    return hidden, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "  def __init__(self, encoding_size, embedding_size, use_cuda=False):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "\n",
    "    self.encoding_size = encoding_size\n",
    "\n",
    "    self.decoder = nn.LSTM(\n",
    "      input_size=embedding_size, \n",
    "      hidden_size=encoding_size,\n",
    "      num_layers=1,\n",
    "      bias=True,\n",
    "      batch_first=True,\n",
    "      dropout=0.1,\n",
    "      bidirectional=False\n",
    "    )\n",
    "\n",
    "    self.dim_linear = nn.Linear(encoding_size, embedding_size)\n",
    "    self.dim_fn = nn.Tanh()    \n",
    "\n",
    "  def forward(self, embeddings, init_hidden, init_memory):\n",
    "\n",
    "    output_, (_, _) = self.decoder(embeddings, (init_hidden, init_memory))\n",
    "    linear = self.dim_linear(output_)\n",
    "    \n",
    "    return self.dim_fn(linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining functions related with transforming data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform each sentence by adding a start and finish sequence for encoder and decoder training\n",
    "def get_transformation(vocabulary, embedding_size):\n",
    "    first_embedding = torch.ones((embedding_size,))\n",
    "    last_embedding = torch.zeros((embedding_size,))\n",
    "\n",
    "    def transform_example(example):\n",
    "        transformed = []\n",
    "        transformed.append(first_embedding)\n",
    "\n",
    "        for idx in example:\n",
    "            transformed.append(vocabulary.vectors[idx])\n",
    "        \n",
    "        transformed.append(last_embedding)\n",
    "        \n",
    "        transformed = torch.stack(transformed)[:15]\n",
    "        transformed = transformed.unsqueeze(0)\n",
    "\n",
    "        return transformed\n",
    "\n",
    "    return transform_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices from word's vectors\n",
    "def get_index_fn(vectors):\n",
    "    def get_index(prediction):\n",
    "        indices = []\n",
    "\n",
    "        for vector in prediction:\n",
    "            result = torch.abs(vectors - vector).norm(2, dim=1)\n",
    "            indices.append(torch.argmin(result))\n",
    "\n",
    "        indices = torch.stack(indices)\n",
    "        return indices\n",
    "\n",
    "    return get_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function recover a sentence from word's indices\n",
    "def get_text_fn(itos):\n",
    "    def get_text(example):\n",
    "        text = []\n",
    "        for idx in example:\n",
    "            text.append(itos[idx])\n",
    "\n",
    "        return ' '.join(text)\n",
    "    return get_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define variables related with loading information and training/validation data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTORS_LOADED = 20000\n",
    "VAL_PARTITION = 0.05 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading word vectors and trainig/validation dataset"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext = torchtext.vocab.FastText(language='en', max_vectors=VECTORS_LOADED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = torchtext.vocab.Vocab(collections.Counter(fasttext.stoi.keys()))\n",
    "vocabulary.set_vectors(fasttext.stoi, fasttext.vectors, fasttext.dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "\n0lines [00:00, ?lines/s]\u001b[A\n803lines [00:00, 8026.10lines/s]\u001b[A\n1623lines [00:00, 8076.53lines/s]\u001b[A\n2459lines [00:00, 8158.12lines/s]\u001b[A\n3245lines [00:00, 8066.04lines/s]\u001b[A\n3918lines [00:00, 7612.03lines/s]\u001b[A\n4715lines [00:00, 7713.86lines/s]\u001b[A\n5444lines [00:00, 7580.26lines/s]\u001b[A\n6306lines [00:00, 7862.64lines/s]\u001b[A\n7146lines [00:00, 8016.30lines/s]\u001b[A\n8003lines [00:01, 8172.54lines/s]\u001b[A\n8825lines [00:01, 8185.90lines/s]\u001b[A\n9668lines [00:01, 8257.41lines/s]\u001b[A\n10499lines [00:01, 8273.11lines/s]\u001b[A\n11340lines [00:01, 8312.72lines/s]\u001b[A\n12206lines [00:01, 8413.45lines/s]\u001b[A\n13099lines [00:01, 8560.92lines/s]\u001b[A\n13958lines [00:01, 8568.27lines/s]\u001b[A\n14814lines [00:01, 8368.94lines/s]\u001b[A\n15708lines [00:01, 8531.51lines/s]\u001b[A\n16563lines [00:02, 8469.85lines/s]\u001b[A\n17462lines [00:02, 8616.38lines/s]\u001b[A\n18325lines [00:02, 8525.76lines/s]\u001b[A\n19179lines [00:02, 8410.64lines/s]\u001b[A\n20065lines [00:02, 8540.22lines/s]\u001b[A\n20953lines [00:02, 8639.36lines/s]\u001b[A\n21834lines [00:02, 8688.46lines/s]\u001b[A\n22733lines [00:02, 8774.42lines/s]\u001b[A\n23631lines [00:02, 8832.89lines/s]\u001b[A\n24540lines [00:02, 8907.17lines/s]\u001b[A\n25432lines [00:03, 8825.56lines/s]\u001b[A\n26316lines [00:03, 8735.59lines/s]\u001b[A\n27198lines [00:03, 8759.08lines/s]\u001b[A\n28075lines [00:03, 8723.94lines/s]\u001b[A\n28948lines [00:03, 8720.09lines/s]\u001b[A\n29821lines [00:03, 8268.19lines/s]\u001b[A\n30653lines [00:03, 8278.07lines/s]\u001b[A\n31529lines [00:03, 8416.07lines/s]\u001b[A\n32374lines [00:03, 8385.38lines/s]\u001b[A\n33215lines [00:03, 8239.16lines/s]\u001b[A\n34116lines [00:04, 8454.32lines/s]\u001b[A\n34965lines [00:04, 5757.53lines/s]\u001b[A\n35811lines [00:04, 6366.63lines/s]\u001b[A\n36690lines [00:04, 6940.47lines/s]\u001b[A\n37559lines [00:04, 7386.20lines/s]\u001b[A\n38412lines [00:04, 7692.72lines/s]\u001b[A\n39233lines [00:04, 7802.77lines/s]\u001b[A\n40107lines [00:04, 8062.09lines/s]\u001b[A\n40941lines [00:05, 7902.91lines/s]\u001b[A\n41752lines [00:05, 7927.92lines/s]\u001b[A\n42571lines [00:05, 8002.75lines/s]\u001b[A\n43388lines [00:05, 8049.70lines/s]\u001b[A\n44201lines [00:05, 7934.66lines/s]\u001b[A\n45033lines [00:05, 8044.55lines/s]\u001b[A\n45917lines [00:05, 8267.01lines/s]\u001b[A\n46749lines [00:05, 8243.22lines/s]\u001b[A\n47577lines [00:05, 8155.86lines/s]\u001b[A\n48395lines [00:05, 8112.33lines/s]\u001b[A\n49261lines [00:06, 8268.61lines/s]\u001b[A\n50158lines [00:06, 8466.57lines/s]\u001b[A\n51063lines [00:06, 8631.72lines/s]\u001b[A\n51929lines [00:06, 8360.44lines/s]\u001b[A\n52784lines [00:06, 8414.66lines/s]\u001b[A\n53629lines [00:06, 8154.42lines/s]\u001b[A\n54458lines [00:06, 8193.84lines/s]\u001b[A\n55280lines [00:06, 8149.79lines/s]\u001b[A\n56150lines [00:06, 8303.82lines/s]\u001b[A\n56983lines [00:06, 7992.57lines/s]\u001b[A\n57838lines [00:07, 8151.44lines/s]\u001b[A\n58657lines [00:07, 8012.25lines/s]\u001b[A\n59470lines [00:07, 8045.34lines/s]\u001b[A\n60333lines [00:07, 8211.73lines/s]\u001b[A\n61198lines [00:07, 8337.94lines/s]\u001b[A\n62098lines [00:07, 8526.09lines/s]\u001b[A\n62954lines [00:07, 8368.35lines/s]\u001b[A\n63819lines [00:07, 8450.41lines/s]\u001b[A\n64677lines [00:07, 8485.75lines/s]\u001b[A\n65528lines [00:07, 8490.81lines/s]\u001b[A\n66379lines [00:08, 8490.55lines/s]\u001b[A\n67284lines [00:08, 8650.76lines/s]\u001b[A\n68151lines [00:08, 8565.68lines/s]\u001b[A\n69028lines [00:08, 8624.12lines/s]\u001b[A\n69892lines [00:08, 8469.64lines/s]\u001b[A\n70779lines [00:08, 8584.52lines/s]\u001b[A\n71639lines [00:08, 8386.18lines/s]\u001b[A\n72489lines [00:08, 8417.60lines/s]\u001b[A\n73333lines [00:08, 8362.86lines/s]\u001b[A\n74177lines [00:09, 8385.46lines/s]\u001b[A\n75039lines [00:09, 8453.23lines/s]\u001b[A\n75904lines [00:09, 8509.41lines/s]\u001b[A\n76756lines [00:09, 8401.70lines/s]\u001b[A\n77597lines [00:09, 8221.84lines/s]\u001b[A\n78486lines [00:09, 8409.52lines/s]\u001b[A\n79411lines [00:09, 8643.29lines/s]\u001b[A\n80332lines [00:09, 8803.86lines/s]\u001b[A\n81253lines [00:09, 8921.41lines/s]\u001b[A\n82148lines [00:09, 8921.94lines/s]\u001b[A\n83042lines [00:10, 8779.04lines/s]\u001b[A\n83923lines [00:10, 8786.44lines/s]\u001b[A\n84843lines [00:10, 8904.16lines/s]\u001b[A\n85746lines [00:10, 8939.82lines/s]\u001b[A\n86641lines [00:10, 8788.20lines/s]\u001b[A\n87545lines [00:10, 8861.52lines/s]\u001b[A\n88433lines [00:10, 8825.83lines/s]\u001b[A\n89328lines [00:10, 8859.90lines/s]\u001b[A\n90215lines [00:10, 8729.34lines/s]\u001b[A\n91095lines [00:10, 8749.16lines/s]\u001b[A\n91971lines [00:11, 8640.19lines/s]\u001b[A\n92836lines [00:11, 8598.17lines/s]\u001b[A\n93697lines [00:11, 8589.95lines/s]\u001b[A\n94577lines [00:11, 8650.68lines/s]\u001b[A\n95443lines [00:11, 8076.89lines/s]\u001b[A\n96349lines [00:11, 8348.43lines/s]\u001b[A\n97192lines [00:11, 8266.22lines/s]\u001b[A\n98025lines [00:11, 5182.02lines/s]\u001b[A\n98800lines [00:12, 5752.48lines/s]\u001b[A\n99621lines [00:12, 6318.56lines/s]\u001b[A\n100498lines [00:12, 6895.27lines/s]\u001b[A\n101409lines [00:12, 7435.81lines/s]\u001b[A\n102265lines [00:12, 7739.12lines/s]\u001b[A\n103094lines [00:12, 7817.61lines/s]\u001b[A\n103930lines [00:12, 7971.72lines/s]\u001b[A\n104755lines [00:12, 8030.52lines/s]\u001b[A\n105602lines [00:12, 8155.63lines/s]\u001b[A\n106469lines [00:12, 8303.10lines/s]\u001b[A\n107319lines [00:13, 8358.51lines/s]\u001b[A\n108163lines [00:13, 8280.49lines/s]\u001b[A\n108997lines [00:13, 8231.70lines/s]\u001b[A\n109856lines [00:13, 8335.07lines/s]\u001b[A\n110715lines [00:13, 8407.15lines/s]\u001b[A\n111558lines [00:13, 8362.10lines/s]\u001b[A\n112401lines [00:13, 8380.53lines/s]\u001b[A\n113241lines [00:13, 8260.36lines/s]\u001b[A\n114069lines [00:13, 8086.37lines/s]\u001b[A\n114911lines [00:14, 8181.03lines/s]\u001b[A\n115731lines [00:14, 8147.89lines/s]\u001b[A\n116603lines [00:14, 8308.97lines/s]\u001b[A\n117467lines [00:14, 8404.82lines/s]\u001b[A\n118309lines [00:14, 8405.41lines/s]\u001b[A\n120000lines [00:14, 8215.41lines/s]\n\n0lines [00:00, ?lines/s]\u001b[A\n793lines [00:00, 7929.80lines/s]\u001b[A\n1651lines [00:00, 8112.48lines/s]\u001b[A\n2514lines [00:00, 8255.71lines/s]\u001b[A\n3277lines [00:00, 8057.10lines/s]\u001b[A\n4074lines [00:00, 8026.08lines/s]\u001b[A\n4939lines [00:00, 8202.15lines/s]\u001b[A\n5790lines [00:00, 8290.15lines/s]\u001b[A\n6669lines [00:00, 8433.34lines/s]\u001b[A\n7600lines [00:00, 8393.09lines/s]\n"
    }
   ],
   "source": [
    "train_dataset, test_dataset = torchtext.datasets.AG_NEWS(ngrams=3, vocab=vocabulary)\n",
    "test_dataset = test_dataset[:int(len(test_dataset) * VAL_PARTITION)]\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining variables related with training"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "EMBEDDING_SIZE = fasttext.dim\n",
    "ENCODING_SIZE = 1024\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining training components"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(EMBEDDING_SIZE, ENCODING_SIZE, use_cuda=USE_CUDA)\n",
    "decoder = Decoder(ENCODING_SIZE, EMBEDDING_SIZE, use_cuda=USE_CUDA)\n",
    "\n",
    "encoder.load_state_dict(torch.load('checkpoints/encoder.pt'))\n",
    "decoder.load_state_dict(torch.load('checkpoints/decoder.pt'))\n",
    "\n",
    "if USE_CUDA:\n",
    "    encoder = encoder.cuda()\n",
    "    decoder = decoder.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    list(encoder.parameters()) + list(decoder.parameters()), \n",
    "    lr=LEARNING_RATE\n",
    ")\n",
    "\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation_fn = get_transformation(vocabulary, EMBEDDING_SIZE)\n",
    "get_text = get_text_fn(vocabulary.itos)\n",
    "get_indices = get_index_fn(vocabulary.vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and validation"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(encoder, decoder, loss_fn, optimizer, batch, use_cuda):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    if use_cuda:\n",
    "      batch = batch.cuda()\n",
    "\n",
    "    encoder.zero_grad()\n",
    "    decoder.zero_grad()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    representation, memory = encoder(batch[:,1:,:])\n",
    "\n",
    "    if use_cuda:\n",
    "      representation = representation.cuda()\n",
    "      memory = memory.cuda()\n",
    "    \n",
    "    decodings = decoder(batch[:,:-1,:], representation, memory)\n",
    "\n",
    "    loss = loss_fn(batch[:,1:,:], decodings)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_step(encoder, decoder, loss_fn, batch, use_cuda):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        if use_cuda:\n",
    "            batch = batch.cuda()\n",
    "\n",
    "        representation, memory = encoder(batch[:,1:,:])\n",
    "\n",
    "        if use_cuda:\n",
    "            representation = representation.cuda()\n",
    "            memory = memory.cuda()\n",
    "        \n",
    "        decodings = decoder(batch[:,:-1,:], representation, memory)\n",
    "\n",
    "        loss = loss_fn(batch[:,1:,:], decodings)\n",
    "\n",
    "        return loss.item(), decodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Loss: 0.0:   0%|          | 0/120000 [00:00<?, ?it/s]"
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [85 x 2048], m2: [1024 x 300] at /tmp/pip-req-build-4baxydiv/aten/src/THC/generic/THCTensorMathBlas.cu:290",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-2aa87bb9500e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUSE_CUDA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-f59968b3e8d2>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(encoder, decoder, loss_fn, optimizer, batch, use_cuda)\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mdecodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepresentation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecodings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ML/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-a117dd79394e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, embeddings, init_hidden, init_memory)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0moutput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_memory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mlinear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ML/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ML/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ML/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1373\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [85 x 2048], m2: [1024 x 300] at /tmp/pip-req-build-4baxydiv/aten/src/THC/generic/THCTensorMathBlas.cu:290"
     ]
    }
   ],
   "source": [
    "# Define steps where examples will be sampled \n",
    "example_step = VAL_PARTITION * len(train_dataset)\n",
    "test_examples = iter(test_dataset)\n",
    "\n",
    "last_val_loss = None\n",
    "\n",
    "# For EPOCHS\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "  # Restart train and validation datasets\n",
    "  examples = iter(train_dataset)\n",
    "  val_examples = iter(test_dataset)\n",
    "\n",
    "  # Progress bar for training dataset\n",
    "  progress_bar = tqdm(range(len(train_dataset)))\n",
    "  train_loss = 0\n",
    "  \n",
    "  # For all data in training dataset\n",
    "  \n",
    "  for batch_idx in progress_bar:\n",
    "\n",
    "    # Add train loss to progress bar\n",
    "    progress_bar.set_description('Loss: {}'.format(train_loss / (batch_idx + 1)))\n",
    "    \n",
    "    # Train step\n",
    "    example = next(examples)[1]\n",
    "    embeddings = transformation_fn(example)\n",
    "\n",
    "    train_loss += train_step(encoder, decoder, loss_fn, optimizer, embeddings, USE_CUDA)\n",
    "\n",
    "\n",
    "    if batch_idx % example_step == 0:\n",
    "      with torch.no_grad():\n",
    "        try:\n",
    "          example = next(test_examples)[1]\n",
    "        except:\n",
    "          test_examples = iter(test_dataset)\n",
    "          example = next(test_examples)[1]\n",
    "\n",
    "        embeddings = transformation_fn(example)\n",
    "\n",
    "        _, decodings = val_step(encoder, decoder, loss_fn, embeddings, USE_CUDA)\n",
    "\n",
    "        print('\\nReal: {}'.format(get_text(example[:15])))\n",
    "        print('Decoded: {}'.format(get_text(get_indices(decodings[0].cpu()))))      \n",
    "\n",
    "\n",
    "  with torch.no_grad():\n",
    "    progress_bar = tqdm(range(len(test_dataset)))\n",
    "    val_loss = 0\n",
    "\n",
    "    for batch_idx in progress_bar:\n",
    "      progress_bar.set_description('Val loss: {}'.format(val_loss / (batch_idx + 1)))\n",
    "      example = next(val_examples)[1]\n",
    "      embeddings = transformation_fn(example)\n",
    "\n",
    "      val_loss += val_step(encoder, decoder, loss_fn, embeddings, USE_CUDA)[0]\n",
    "       \n",
    "\n",
    "    if last_val_loss is None or val_loss < last_val_loss:\n",
    "      last_val_loss = val_loss\n",
    "\n",
    "      torch.save(encoder.state_dict(), 'checkpoints/encoder_{}.pt'.format(epoch))\n",
    "      torch.save(decoder.state_dict(), 'checkpoints/decoder_{}.pt'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}