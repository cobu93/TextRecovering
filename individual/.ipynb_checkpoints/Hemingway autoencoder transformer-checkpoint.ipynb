{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "\n",
    "from text.utils import save_text_state, load_text_state, build_text_state, load_books\n",
    "from model.utils import get_device, load_model_state, save_model_state, build_model_state\n",
    "\n",
    "from text_model import TextModel, get_loss_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify if CUDA is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = get_device()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading word vectors and training/validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_CHECKPOINTS = True\n",
    "\n",
    "CHECKPOINT_BASE = 'checkpoints'\n",
    "DATASET_FILENAME = '../datasets/books.pk'\n",
    "AUTHOR = 'Ernest Hemingway'\n",
    "\n",
    "VAL_PARTITION = 0.3 \n",
    "MAX_SENTENCE_LENGTH = 15\n",
    "MIN_SENTENCE_LENGTH = 5\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SENTENCE_LENGTH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f0ec5304a303>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_text_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHECKPOINT_BASE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAUTHOR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_text.pk'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Checkpoint loaded'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documentos/CIC/DCC/Papers/Style Transfer/TextRecovering/utils/text/utils.py\u001b[0m in \u001b[0;36mload_text_state\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_text_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m         \u001b[0mtext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'checkpoints/Ernest Hemingway_text.pk'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f0ec5304a303>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mbooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_books\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_FILENAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAUTHOR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_text_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSENTENCE_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVAL_PARTITION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAUTHOR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No checkpoint found. New partition.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Included books:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SENTENCE_LENGTH' is not defined"
     ]
    }
   ],
   "source": [
    "# Try loading dataset, if it doesn't exists then creates it\n",
    "try:\n",
    "    if not LOAD_CHECKPOINTS:\n",
    "        print('New training required')\n",
    "        raise Error('New training required')\n",
    "        \n",
    "    vocab, train_dataset, test_dataset = load_text_state(os.path.join(CHECKPOINT_BASE, AUTHOR + '_text.pk'))    \n",
    "    print('Checkpoint loaded')\n",
    "except:\n",
    "    books = load_books(DATASET_FILENAME, author=AUTHOR)\n",
    "    vocab, train_dataset, test_dataset = build_text_state(books, MIN_SENTENCE_LENGTH, MAX_SENTENCE_LENGTH, VAL_PARTITION, vocab_name=AUTHOR)  \n",
    "    print('No checkpoint found. New partition.')\n",
    "    print('Included books:')\n",
    "    for book in books:\n",
    "        print('\\t', book)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_text_state(os.path.join(CHECKPOINT_BASE, AUTHOR + '_text.pk'), vocab, train_dataset, test_dataset) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(vocab)\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "BETA_1 = 0.1\n",
    "BETA_2 = 0.999\n",
    "EPOCHS = 50\n",
    "NUM_HEADS = 8\n",
    "ENCODER_LAYERS = 1\n",
    "DECODER_LAYERS = 1\n",
    "EMBEDDING_SIZE = 512\n",
    "FF_DIM = 1024\n",
    "DROPOUT=0.1\n",
    "STEP_LR_DECAY = 15\n",
    "LR_FACTOR_DECAY = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextModel(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embedding_size=EMBEDDING_SIZE, \n",
    "    num_heads=NUM_HEADS, \n",
    "    encoder_layers=ENCODER_LAYERS, \n",
    "    decoder_layers=DECODER_LAYERS, \n",
    "    dim_feedforward=FF_DIM,\n",
    "    dropout=DROPOUT\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(model.parameters()), \n",
    "    lr=LEARNING_RATE,\n",
    "    betas=(BETA_1, BETA_2)\n",
    ")\n",
    "\n",
    "loss_fn = get_loss_function()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=STEP_LR_DECAY, gamma=LR_FACTOR_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if not LOAD_CHECKPOINTS:\n",
    "        print('New training required')\n",
    "        raise Error('New training required')\n",
    "        \n",
    "    model, optimizer, last_epoch, train_loss_history, val_loss_history, best_val_loss = load_model_state(\n",
    "        os.path.join(CHECKPOINT_BASE, AUTHOR + '_best.pt'), \n",
    "        model, \n",
    "        optimizer\n",
    "    )\n",
    "    \n",
    "    for state in optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                state[k] = v.to(device)\n",
    "\n",
    "    print('Checkpoint loaded')\n",
    "    \n",
    "except:\n",
    "    last_epoch, train_loss_history, val_loss_history, best_val_loss = build_model_state()\n",
    "    print('No checkpoints found. New training.')\n",
    "    pass\n",
    "\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(transformer, loss_fn, optimizer, batch):\n",
    "    transformer.train()\n",
    "\n",
    "    transformer.zero_grad()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    decodings = transformer(batch[:,1:], batch[:,:-1])\n",
    "    \n",
    "    loss = loss_fn(batch[:,1:], decodings)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_step(transformer, loss_fn, batch):\n",
    "    transformer.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        decodings = transformer(batch[:,1:], batch[:,:-1])\n",
    "        loss = loss_fn(batch[:,1:], decodings)\n",
    "\n",
    "        return loss.item(), decodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Define steps where examples will be sampled \n",
    "test_examples = iter(test_dataloader)\n",
    "\n",
    "# For EPOCHS\n",
    "for epoch in range(last_epoch + 1, EPOCHS + last_epoch):\n",
    "    \n",
    "    print('*************************** EPOCH {} ***************************'.format(epoch))\n",
    "\n",
    "    # Restart train dataset\n",
    "    examples = iter(train_dataloader)\n",
    "\n",
    "    # Progress bar for training dataset\n",
    "    progress_bar = tqdm(range(len(train_dataloader)))\n",
    "    train_loss = 0\n",
    "\n",
    "    # For all data in training dataset\n",
    "    # Training\n",
    "    for batch_idx in progress_bar:\n",
    "\n",
    "        # Add train loss to progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'tr_loss': train_loss / (batch_idx + 1), \n",
    "            'lr': scheduler.get_lr()[0]\n",
    "        })\n",
    "\n",
    "        # Train step\n",
    "        example = next(examples).to(device)\n",
    "        train_loss += train_step(model, loss_fn, optimizer, example)\n",
    "\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            try:\n",
    "                example = next(test_examples).to(device)\n",
    "            except:\n",
    "                test_examples = iter(test_dataloader)\n",
    "                example = next(test_examples).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                _, decodings = val_step(model, loss_fn, example)\n",
    "                decodings = torch.argmax(decodings[0], dim=-1).cpu().numpy()\n",
    "\n",
    "            print('\\nReal: {}'.format( ' '.join(vocab.to_words(example[0][1:].cpu().numpy())) ))\n",
    "            print('Decoded: {}'.format( ' '.join(vocab.to_words(decodings))))\n",
    "\n",
    "    # Validation\n",
    "    val_examples = iter(test_dataloader)\n",
    "    progress_bar = tqdm(range(len(test_dataloader)))\n",
    "    val_loss = 0\n",
    "\n",
    "    for batch_idx in progress_bar:\n",
    "        progress_bar.set_postfix({\n",
    "            'val_loss': val_loss / (batch_idx + 1)\n",
    "        })\n",
    "        \n",
    "        example = next(val_examples).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_loss += val_step(model, loss_fn, example)[0]\n",
    "    \n",
    "    # Save losses in history\n",
    "    train_loss_history.append(train_loss / len(train_dataloader))\n",
    "    val_loss_history.append(val_loss / len(test_dataloader))\n",
    "    \n",
    "    # If this model is best than other\n",
    "    if best_val_loss is None or val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        \n",
    "        save_model_state(\n",
    "            os.path.join(CHECKPOINT_BASE, AUTHOR + '_best.pt'), \n",
    "            model, \n",
    "            optimizer, \n",
    "            epoch, \n",
    "            train_loss_history, \n",
    "            val_loss_history, \n",
    "            best_val_loss\n",
    "        )\n",
    "        \n",
    "    # Save last model\n",
    "    save_model_state(\n",
    "        os.path.join(CHECKPOINT_BASE, AUTHOR + '_last.pt'), \n",
    "        model, \n",
    "        optimizer, \n",
    "        epoch, \n",
    "        train_loss_history, \n",
    "        val_loss_history, \n",
    "        best_val_loss\n",
    "    )    \n",
    "    \n",
    "    scheduler.step()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [epoch for epoch in range(len(train_loss_history))]\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and validation loss')\n",
    "\n",
    "plt.plot(x, train_loss_history, label='Training loss')\n",
    "plt.plot(x, val_loss_history, label='Validation loss')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "argmin_val = np.argmin(val_loss_history)\n",
    "print('Least validation loss: {} in epoch {}'.format(val_loss_history[argmin_val], argmin_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
